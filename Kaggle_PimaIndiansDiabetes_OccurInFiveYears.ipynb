{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "041b1837",
   "metadata": {},
   "source": [
    "## 1. 下載資料集並觀察資料維度\n",
    "本資料集共有768筆資料, 每筆資料有8個特徵; 資料集最後一欄位為5年後不發病(0)與發病(1)的標籤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb76e6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (768, 8)\n",
      "label.shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# 可從kaggle網站下載此資料集 https://www.kaggle.com/datasets/kumargh/pimaindiansdiabetescsv?resource=download\n",
    "dataset = np.loadtxt(\"./pima_indians_diabetes.csv\", delimiter=\",\")\n",
    "data = dataset[:, 0:8]\n",
    "label = dataset[:, 8]\n",
    "\n",
    "print(\"data.shape:\", data.shape)\n",
    "print(\"label.shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d64f032",
   "metadata": {},
   "source": [
    "## 2. 創造網路模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b80f637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 253\n",
      "Trainable params: 253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955829f3",
   "metadata": {},
   "source": [
    "## 3. 編譯與訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "457e7c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "62/62 - 1s - loss: 2.0577 - accuracy: 0.5749 - val_loss: 1.0439 - val_accuracy: 0.6104\n",
      "Epoch 2/150\n",
      "62/62 - 0s - loss: 0.8918 - accuracy: 0.6450 - val_loss: 0.9238 - val_accuracy: 0.5974\n",
      "Epoch 3/150\n",
      "62/62 - 0s - loss: 0.7947 - accuracy: 0.6384 - val_loss: 0.8354 - val_accuracy: 0.5649\n",
      "Epoch 4/150\n",
      "62/62 - 0s - loss: 0.7493 - accuracy: 0.6482 - val_loss: 0.7968 - val_accuracy: 0.6234\n",
      "Epoch 5/150\n",
      "62/62 - 0s - loss: 0.7161 - accuracy: 0.6401 - val_loss: 0.7410 - val_accuracy: 0.6558\n",
      "Epoch 6/150\n",
      "62/62 - 0s - loss: 0.6857 - accuracy: 0.6629 - val_loss: 0.7100 - val_accuracy: 0.6104\n",
      "Epoch 7/150\n",
      "62/62 - 0s - loss: 0.6603 - accuracy: 0.6645 - val_loss: 0.7033 - val_accuracy: 0.5649\n",
      "Epoch 8/150\n",
      "62/62 - 0s - loss: 0.6520 - accuracy: 0.6629 - val_loss: 0.7320 - val_accuracy: 0.5195\n",
      "Epoch 9/150\n",
      "62/62 - 0s - loss: 0.6490 - accuracy: 0.6775 - val_loss: 0.6952 - val_accuracy: 0.5455\n",
      "Epoch 10/150\n",
      "62/62 - 0s - loss: 0.6507 - accuracy: 0.6661 - val_loss: 0.6685 - val_accuracy: 0.6818\n",
      "Epoch 11/150\n",
      "62/62 - 0s - loss: 0.6349 - accuracy: 0.6759 - val_loss: 0.6959 - val_accuracy: 0.6558\n",
      "Epoch 12/150\n",
      "62/62 - 0s - loss: 0.6338 - accuracy: 0.6726 - val_loss: 0.6597 - val_accuracy: 0.6753\n",
      "Epoch 13/150\n",
      "62/62 - 0s - loss: 0.6247 - accuracy: 0.6710 - val_loss: 0.6635 - val_accuracy: 0.5974\n",
      "Epoch 14/150\n",
      "62/62 - 0s - loss: 0.6180 - accuracy: 0.6726 - val_loss: 0.6594 - val_accuracy: 0.6688\n",
      "Epoch 15/150\n",
      "62/62 - 0s - loss: 0.6257 - accuracy: 0.6645 - val_loss: 0.6539 - val_accuracy: 0.6558\n",
      "Epoch 16/150\n",
      "62/62 - 0s - loss: 0.6154 - accuracy: 0.6726 - val_loss: 0.6610 - val_accuracy: 0.6104\n",
      "Epoch 17/150\n",
      "62/62 - 0s - loss: 0.6111 - accuracy: 0.6857 - val_loss: 0.6412 - val_accuracy: 0.6818\n",
      "Epoch 18/150\n",
      "62/62 - 0s - loss: 0.6023 - accuracy: 0.7085 - val_loss: 0.6387 - val_accuracy: 0.6753\n",
      "Epoch 19/150\n",
      "62/62 - 0s - loss: 0.6039 - accuracy: 0.6824 - val_loss: 0.6415 - val_accuracy: 0.6558\n",
      "Epoch 20/150\n",
      "62/62 - 0s - loss: 0.5996 - accuracy: 0.7101 - val_loss: 0.6310 - val_accuracy: 0.6558\n",
      "Epoch 21/150\n",
      "62/62 - 0s - loss: 0.5966 - accuracy: 0.6987 - val_loss: 0.6253 - val_accuracy: 0.6558\n",
      "Epoch 22/150\n",
      "62/62 - 0s - loss: 0.5958 - accuracy: 0.6987 - val_loss: 0.6120 - val_accuracy: 0.6948\n",
      "Epoch 23/150\n",
      "62/62 - 0s - loss: 0.5938 - accuracy: 0.7020 - val_loss: 0.6144 - val_accuracy: 0.6818\n",
      "Epoch 24/150\n",
      "62/62 - 0s - loss: 0.6026 - accuracy: 0.7182 - val_loss: 0.6114 - val_accuracy: 0.6818\n",
      "Epoch 25/150\n",
      "62/62 - 0s - loss: 0.5888 - accuracy: 0.7117 - val_loss: 0.6076 - val_accuracy: 0.7078\n",
      "Epoch 26/150\n",
      "62/62 - 0s - loss: 0.5786 - accuracy: 0.7166 - val_loss: 0.5993 - val_accuracy: 0.6883\n",
      "Epoch 27/150\n",
      "62/62 - 0s - loss: 0.5755 - accuracy: 0.7166 - val_loss: 0.5965 - val_accuracy: 0.6883\n",
      "Epoch 28/150\n",
      "62/62 - 0s - loss: 0.5784 - accuracy: 0.7166 - val_loss: 0.6352 - val_accuracy: 0.6753\n",
      "Epoch 29/150\n",
      "62/62 - 0s - loss: 0.5760 - accuracy: 0.7068 - val_loss: 0.6041 - val_accuracy: 0.6753\n",
      "Epoch 30/150\n",
      "62/62 - 0s - loss: 0.5676 - accuracy: 0.7231 - val_loss: 0.6109 - val_accuracy: 0.6883\n",
      "Epoch 31/150\n",
      "62/62 - 0s - loss: 0.5692 - accuracy: 0.7264 - val_loss: 0.5936 - val_accuracy: 0.6948\n",
      "Epoch 32/150\n",
      "62/62 - 0s - loss: 0.5815 - accuracy: 0.7427 - val_loss: 0.6120 - val_accuracy: 0.6753\n",
      "Epoch 33/150\n",
      "62/62 - 0s - loss: 0.5605 - accuracy: 0.7378 - val_loss: 0.5874 - val_accuracy: 0.6623\n",
      "Epoch 34/150\n",
      "62/62 - 0s - loss: 0.5621 - accuracy: 0.7410 - val_loss: 0.6150 - val_accuracy: 0.6818\n",
      "Epoch 35/150\n",
      "62/62 - 0s - loss: 0.5707 - accuracy: 0.7231 - val_loss: 0.5920 - val_accuracy: 0.6623\n",
      "Epoch 36/150\n",
      "62/62 - 0s - loss: 0.5727 - accuracy: 0.7264 - val_loss: 0.5987 - val_accuracy: 0.6948\n",
      "Epoch 37/150\n",
      "62/62 - 0s - loss: 0.5705 - accuracy: 0.7394 - val_loss: 0.5902 - val_accuracy: 0.6753\n",
      "Epoch 38/150\n",
      "62/62 - 0s - loss: 0.5564 - accuracy: 0.7280 - val_loss: 0.6161 - val_accuracy: 0.6623\n",
      "Epoch 39/150\n",
      "62/62 - 0s - loss: 0.5681 - accuracy: 0.7199 - val_loss: 0.5865 - val_accuracy: 0.6623\n",
      "Epoch 40/150\n",
      "62/62 - 0s - loss: 0.5520 - accuracy: 0.7329 - val_loss: 0.6392 - val_accuracy: 0.6753\n",
      "Epoch 41/150\n",
      "62/62 - 0s - loss: 0.5561 - accuracy: 0.7394 - val_loss: 0.5782 - val_accuracy: 0.6688\n",
      "Epoch 42/150\n",
      "62/62 - 0s - loss: 0.5579 - accuracy: 0.7345 - val_loss: 0.6235 - val_accuracy: 0.6753\n",
      "Epoch 43/150\n",
      "62/62 - 0s - loss: 0.5612 - accuracy: 0.7362 - val_loss: 0.5894 - val_accuracy: 0.6818\n",
      "Epoch 44/150\n",
      "62/62 - 0s - loss: 0.5519 - accuracy: 0.7182 - val_loss: 0.5819 - val_accuracy: 0.6688\n",
      "Epoch 45/150\n",
      "62/62 - 0s - loss: 0.5560 - accuracy: 0.7410 - val_loss: 0.5821 - val_accuracy: 0.6558\n",
      "Epoch 46/150\n",
      "62/62 - 0s - loss: 0.5540 - accuracy: 0.7329 - val_loss: 0.5949 - val_accuracy: 0.6623\n",
      "Epoch 47/150\n",
      "62/62 - 0s - loss: 0.5452 - accuracy: 0.7443 - val_loss: 0.6012 - val_accuracy: 0.6688\n",
      "Epoch 48/150\n",
      "62/62 - 0s - loss: 0.5385 - accuracy: 0.7362 - val_loss: 0.6151 - val_accuracy: 0.6948\n",
      "Epoch 49/150\n",
      "62/62 - 0s - loss: 0.5502 - accuracy: 0.7378 - val_loss: 0.6120 - val_accuracy: 0.6883\n",
      "Epoch 50/150\n",
      "62/62 - 0s - loss: 0.5496 - accuracy: 0.7427 - val_loss: 0.5854 - val_accuracy: 0.6818\n",
      "Epoch 51/150\n",
      "62/62 - 0s - loss: 0.5481 - accuracy: 0.7231 - val_loss: 0.6930 - val_accuracy: 0.6623\n",
      "Epoch 52/150\n",
      "62/62 - 0s - loss: 0.5523 - accuracy: 0.7329 - val_loss: 0.5879 - val_accuracy: 0.6558\n",
      "Epoch 53/150\n",
      "62/62 - 0s - loss: 0.5492 - accuracy: 0.7362 - val_loss: 0.5936 - val_accuracy: 0.6948\n",
      "Epoch 54/150\n",
      "62/62 - 0s - loss: 0.5388 - accuracy: 0.7459 - val_loss: 0.5902 - val_accuracy: 0.6623\n",
      "Epoch 55/150\n",
      "62/62 - 0s - loss: 0.5402 - accuracy: 0.7459 - val_loss: 0.5809 - val_accuracy: 0.6688\n",
      "Epoch 56/150\n",
      "62/62 - 0s - loss: 0.5453 - accuracy: 0.7492 - val_loss: 0.5935 - val_accuracy: 0.6753\n",
      "Epoch 57/150\n",
      "62/62 - 0s - loss: 0.5330 - accuracy: 0.7590 - val_loss: 0.6206 - val_accuracy: 0.6883\n",
      "Epoch 58/150\n",
      "62/62 - 0s - loss: 0.5503 - accuracy: 0.7264 - val_loss: 0.5730 - val_accuracy: 0.7013\n",
      "Epoch 59/150\n",
      "62/62 - 0s - loss: 0.5319 - accuracy: 0.7443 - val_loss: 0.5785 - val_accuracy: 0.6883\n",
      "Epoch 60/150\n",
      "62/62 - 0s - loss: 0.5354 - accuracy: 0.7606 - val_loss: 0.5704 - val_accuracy: 0.7078\n",
      "Epoch 61/150\n",
      "62/62 - 0s - loss: 0.5336 - accuracy: 0.7492 - val_loss: 0.5823 - val_accuracy: 0.6818\n",
      "Epoch 62/150\n",
      "62/62 - 0s - loss: 0.5357 - accuracy: 0.7590 - val_loss: 0.5825 - val_accuracy: 0.6948\n",
      "Epoch 63/150\n",
      "62/62 - 0s - loss: 0.5379 - accuracy: 0.7410 - val_loss: 0.5790 - val_accuracy: 0.6883\n",
      "Epoch 64/150\n",
      "62/62 - 0s - loss: 0.5288 - accuracy: 0.7508 - val_loss: 0.5831 - val_accuracy: 0.6948\n",
      "Epoch 65/150\n",
      "62/62 - 0s - loss: 0.5291 - accuracy: 0.7427 - val_loss: 0.5752 - val_accuracy: 0.7078\n",
      "Epoch 66/150\n",
      "62/62 - 0s - loss: 0.5280 - accuracy: 0.7410 - val_loss: 0.5720 - val_accuracy: 0.7013\n",
      "Epoch 67/150\n",
      "62/62 - 0s - loss: 0.5327 - accuracy: 0.7459 - val_loss: 0.5845 - val_accuracy: 0.6948\n",
      "Epoch 68/150\n",
      "62/62 - 0s - loss: 0.5393 - accuracy: 0.7443 - val_loss: 0.5844 - val_accuracy: 0.6883\n",
      "Epoch 69/150\n",
      "62/62 - 0s - loss: 0.5286 - accuracy: 0.7606 - val_loss: 0.5925 - val_accuracy: 0.7013\n",
      "Epoch 70/150\n",
      "62/62 - 0s - loss: 0.5386 - accuracy: 0.7296 - val_loss: 0.5761 - val_accuracy: 0.7013\n",
      "Epoch 71/150\n",
      "62/62 - 0s - loss: 0.5258 - accuracy: 0.7655 - val_loss: 0.5827 - val_accuracy: 0.6818\n",
      "Epoch 72/150\n",
      "62/62 - 0s - loss: 0.5367 - accuracy: 0.7476 - val_loss: 0.5963 - val_accuracy: 0.6753\n",
      "Epoch 73/150\n",
      "62/62 - 0s - loss: 0.5334 - accuracy: 0.7410 - val_loss: 0.5968 - val_accuracy: 0.7143\n",
      "Epoch 74/150\n",
      "62/62 - 0s - loss: 0.5319 - accuracy: 0.7492 - val_loss: 0.5765 - val_accuracy: 0.7078\n",
      "Epoch 75/150\n",
      "62/62 - 0s - loss: 0.5309 - accuracy: 0.7410 - val_loss: 0.5759 - val_accuracy: 0.7143\n",
      "Epoch 76/150\n",
      "62/62 - 0s - loss: 0.5214 - accuracy: 0.7638 - val_loss: 0.6305 - val_accuracy: 0.6948\n",
      "Epoch 77/150\n",
      "62/62 - 0s - loss: 0.5273 - accuracy: 0.7443 - val_loss: 0.5950 - val_accuracy: 0.6688\n",
      "Epoch 78/150\n",
      "62/62 - 0s - loss: 0.5245 - accuracy: 0.7492 - val_loss: 0.5770 - val_accuracy: 0.7078\n",
      "Epoch 79/150\n",
      "62/62 - 0s - loss: 0.5351 - accuracy: 0.7476 - val_loss: 0.5791 - val_accuracy: 0.7403\n",
      "Epoch 80/150\n",
      "62/62 - 0s - loss: 0.5263 - accuracy: 0.7427 - val_loss: 0.5800 - val_accuracy: 0.6883\n",
      "Epoch 81/150\n",
      "62/62 - 0s - loss: 0.5224 - accuracy: 0.7573 - val_loss: 0.5737 - val_accuracy: 0.7078\n",
      "Epoch 82/150\n",
      "62/62 - 0s - loss: 0.5307 - accuracy: 0.7476 - val_loss: 0.5810 - val_accuracy: 0.7078\n",
      "Epoch 83/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 - 0s - loss: 0.5145 - accuracy: 0.7492 - val_loss: 0.5912 - val_accuracy: 0.7013\n",
      "Epoch 84/150\n",
      "62/62 - 0s - loss: 0.5118 - accuracy: 0.7541 - val_loss: 0.6010 - val_accuracy: 0.6948\n",
      "Epoch 85/150\n",
      "62/62 - 0s - loss: 0.5159 - accuracy: 0.7606 - val_loss: 0.6479 - val_accuracy: 0.6753\n",
      "Epoch 86/150\n",
      "62/62 - 0s - loss: 0.5294 - accuracy: 0.7362 - val_loss: 0.5712 - val_accuracy: 0.7208\n",
      "Epoch 87/150\n",
      "62/62 - 0s - loss: 0.5162 - accuracy: 0.7687 - val_loss: 0.5860 - val_accuracy: 0.7013\n",
      "Epoch 88/150\n",
      "62/62 - 0s - loss: 0.5169 - accuracy: 0.7606 - val_loss: 0.5687 - val_accuracy: 0.7078\n",
      "Epoch 89/150\n",
      "62/62 - 0s - loss: 0.5218 - accuracy: 0.7508 - val_loss: 0.5969 - val_accuracy: 0.6948\n",
      "Epoch 90/150\n",
      "62/62 - 0s - loss: 0.5173 - accuracy: 0.7541 - val_loss: 0.5764 - val_accuracy: 0.7078\n",
      "Epoch 91/150\n",
      "62/62 - 0s - loss: 0.5099 - accuracy: 0.7508 - val_loss: 0.5901 - val_accuracy: 0.6948\n",
      "Epoch 92/150\n",
      "62/62 - 0s - loss: 0.5126 - accuracy: 0.7638 - val_loss: 0.5762 - val_accuracy: 0.7403\n",
      "Epoch 93/150\n",
      "62/62 - 0s - loss: 0.5044 - accuracy: 0.7655 - val_loss: 0.5735 - val_accuracy: 0.7078\n",
      "Epoch 94/150\n",
      "62/62 - 0s - loss: 0.5062 - accuracy: 0.7492 - val_loss: 0.5669 - val_accuracy: 0.7338\n",
      "Epoch 95/150\n",
      "62/62 - 0s - loss: 0.5162 - accuracy: 0.7573 - val_loss: 0.5697 - val_accuracy: 0.7078\n",
      "Epoch 96/150\n",
      "62/62 - 0s - loss: 0.5079 - accuracy: 0.7590 - val_loss: 0.6162 - val_accuracy: 0.6948\n",
      "Epoch 97/150\n",
      "62/62 - 0s - loss: 0.5077 - accuracy: 0.7590 - val_loss: 0.5851 - val_accuracy: 0.7143\n",
      "Epoch 98/150\n",
      "62/62 - 0s - loss: 0.5278 - accuracy: 0.7541 - val_loss: 0.6021 - val_accuracy: 0.6753\n",
      "Epoch 99/150\n",
      "62/62 - 0s - loss: 0.5013 - accuracy: 0.7736 - val_loss: 0.5866 - val_accuracy: 0.7013\n",
      "Epoch 100/150\n",
      "62/62 - 0s - loss: 0.5109 - accuracy: 0.7524 - val_loss: 0.5758 - val_accuracy: 0.7143\n",
      "Epoch 101/150\n",
      "62/62 - 0s - loss: 0.5382 - accuracy: 0.7427 - val_loss: 0.5560 - val_accuracy: 0.7468\n",
      "Epoch 102/150\n",
      "62/62 - 0s - loss: 0.5096 - accuracy: 0.7785 - val_loss: 0.5752 - val_accuracy: 0.7208\n",
      "Epoch 103/150\n",
      "62/62 - 0s - loss: 0.5086 - accuracy: 0.7622 - val_loss: 0.5673 - val_accuracy: 0.7208\n",
      "Epoch 104/150\n",
      "62/62 - 0s - loss: 0.5012 - accuracy: 0.7752 - val_loss: 0.5712 - val_accuracy: 0.7273\n",
      "Epoch 105/150\n",
      "62/62 - 0s - loss: 0.5055 - accuracy: 0.7704 - val_loss: 0.5653 - val_accuracy: 0.7273\n",
      "Epoch 106/150\n",
      "62/62 - 0s - loss: 0.5090 - accuracy: 0.7410 - val_loss: 0.6294 - val_accuracy: 0.6753\n",
      "Epoch 107/150\n",
      "62/62 - 0s - loss: 0.5114 - accuracy: 0.7573 - val_loss: 0.5774 - val_accuracy: 0.7078\n",
      "Epoch 108/150\n",
      "62/62 - 0s - loss: 0.5012 - accuracy: 0.7622 - val_loss: 0.6047 - val_accuracy: 0.6818\n",
      "Epoch 109/150\n",
      "62/62 - 0s - loss: 0.4961 - accuracy: 0.7720 - val_loss: 0.5726 - val_accuracy: 0.7338\n",
      "Epoch 110/150\n",
      "62/62 - 0s - loss: 0.5016 - accuracy: 0.7671 - val_loss: 0.5745 - val_accuracy: 0.7143\n",
      "Epoch 111/150\n",
      "62/62 - 0s - loss: 0.5090 - accuracy: 0.7590 - val_loss: 0.6325 - val_accuracy: 0.6623\n",
      "Epoch 112/150\n",
      "62/62 - 0s - loss: 0.4958 - accuracy: 0.7720 - val_loss: 0.5719 - val_accuracy: 0.7208\n",
      "Epoch 113/150\n",
      "62/62 - 0s - loss: 0.4979 - accuracy: 0.7622 - val_loss: 0.5672 - val_accuracy: 0.7208\n",
      "Epoch 114/150\n",
      "62/62 - 0s - loss: 0.5077 - accuracy: 0.7557 - val_loss: 0.5648 - val_accuracy: 0.7208\n",
      "Epoch 115/150\n",
      "62/62 - 0s - loss: 0.5120 - accuracy: 0.7573 - val_loss: 0.5882 - val_accuracy: 0.6883\n",
      "Epoch 116/150\n",
      "62/62 - 0s - loss: 0.5004 - accuracy: 0.7573 - val_loss: 0.5690 - val_accuracy: 0.7338\n",
      "Epoch 117/150\n",
      "62/62 - 0s - loss: 0.5003 - accuracy: 0.7720 - val_loss: 0.5942 - val_accuracy: 0.7143\n",
      "Epoch 118/150\n",
      "62/62 - 0s - loss: 0.4997 - accuracy: 0.7638 - val_loss: 0.5808 - val_accuracy: 0.7013\n",
      "Epoch 119/150\n",
      "62/62 - 0s - loss: 0.4903 - accuracy: 0.7785 - val_loss: 0.5689 - val_accuracy: 0.7208\n",
      "Epoch 120/150\n",
      "62/62 - 0s - loss: 0.5030 - accuracy: 0.7638 - val_loss: 0.5912 - val_accuracy: 0.7143\n",
      "Epoch 121/150\n",
      "62/62 - 0s - loss: 0.4956 - accuracy: 0.7590 - val_loss: 0.5650 - val_accuracy: 0.7143\n",
      "Epoch 122/150\n",
      "62/62 - 0s - loss: 0.4956 - accuracy: 0.7606 - val_loss: 0.5645 - val_accuracy: 0.7208\n",
      "Epoch 123/150\n",
      "62/62 - 0s - loss: 0.4958 - accuracy: 0.7720 - val_loss: 0.5823 - val_accuracy: 0.7013\n",
      "Epoch 124/150\n",
      "62/62 - 0s - loss: 0.4902 - accuracy: 0.7704 - val_loss: 0.5639 - val_accuracy: 0.7078\n",
      "Epoch 125/150\n",
      "62/62 - 0s - loss: 0.4883 - accuracy: 0.7671 - val_loss: 0.5609 - val_accuracy: 0.7078\n",
      "Epoch 126/150\n",
      "62/62 - 0s - loss: 0.4957 - accuracy: 0.7508 - val_loss: 0.5597 - val_accuracy: 0.7338\n",
      "Epoch 127/150\n",
      "62/62 - 0s - loss: 0.4927 - accuracy: 0.7671 - val_loss: 0.5678 - val_accuracy: 0.7273\n",
      "Epoch 128/150\n",
      "62/62 - 0s - loss: 0.5046 - accuracy: 0.7590 - val_loss: 0.6137 - val_accuracy: 0.6623\n",
      "Epoch 129/150\n",
      "62/62 - 0s - loss: 0.4843 - accuracy: 0.7655 - val_loss: 0.6258 - val_accuracy: 0.6883\n",
      "Epoch 130/150\n",
      "62/62 - 0s - loss: 0.4930 - accuracy: 0.7736 - val_loss: 0.5681 - val_accuracy: 0.7338\n",
      "Epoch 131/150\n",
      "62/62 - 0s - loss: 0.4872 - accuracy: 0.7655 - val_loss: 0.5685 - val_accuracy: 0.7208\n",
      "Epoch 132/150\n",
      "62/62 - 0s - loss: 0.4923 - accuracy: 0.7720 - val_loss: 0.5561 - val_accuracy: 0.7338\n",
      "Epoch 133/150\n",
      "62/62 - 0s - loss: 0.4914 - accuracy: 0.7606 - val_loss: 0.5641 - val_accuracy: 0.7273\n",
      "Epoch 134/150\n",
      "62/62 - 0s - loss: 0.5013 - accuracy: 0.7638 - val_loss: 0.6142 - val_accuracy: 0.6883\n",
      "Epoch 135/150\n",
      "62/62 - 0s - loss: 0.5174 - accuracy: 0.7590 - val_loss: 0.6425 - val_accuracy: 0.6883\n",
      "Epoch 136/150\n",
      "62/62 - 0s - loss: 0.5215 - accuracy: 0.7557 - val_loss: 0.6097 - val_accuracy: 0.6818\n",
      "Epoch 137/150\n",
      "62/62 - 0s - loss: 0.4968 - accuracy: 0.7557 - val_loss: 0.5735 - val_accuracy: 0.7013\n",
      "Epoch 138/150\n",
      "62/62 - 0s - loss: 0.4884 - accuracy: 0.7801 - val_loss: 0.5594 - val_accuracy: 0.7273\n",
      "Epoch 139/150\n",
      "62/62 - 0s - loss: 0.4853 - accuracy: 0.7769 - val_loss: 0.6012 - val_accuracy: 0.7078\n",
      "Epoch 140/150\n",
      "62/62 - 0s - loss: 0.4957 - accuracy: 0.7524 - val_loss: 0.5811 - val_accuracy: 0.7208\n",
      "Epoch 141/150\n",
      "62/62 - 0s - loss: 0.4875 - accuracy: 0.7818 - val_loss: 0.5787 - val_accuracy: 0.7208\n",
      "Epoch 142/150\n",
      "62/62 - 0s - loss: 0.4825 - accuracy: 0.7720 - val_loss: 0.5729 - val_accuracy: 0.7208\n",
      "Epoch 143/150\n",
      "62/62 - 0s - loss: 0.4773 - accuracy: 0.7769 - val_loss: 0.5687 - val_accuracy: 0.7208\n",
      "Epoch 144/150\n",
      "62/62 - 0s - loss: 0.4950 - accuracy: 0.7638 - val_loss: 0.6080 - val_accuracy: 0.7143\n",
      "Epoch 145/150\n",
      "62/62 - 0s - loss: 0.4907 - accuracy: 0.7818 - val_loss: 0.5665 - val_accuracy: 0.7468\n",
      "Epoch 146/150\n",
      "62/62 - 0s - loss: 0.4856 - accuracy: 0.7736 - val_loss: 0.5697 - val_accuracy: 0.7338\n",
      "Epoch 147/150\n",
      "62/62 - 0s - loss: 0.4884 - accuracy: 0.7524 - val_loss: 0.5722 - val_accuracy: 0.7208\n",
      "Epoch 148/150\n",
      "62/62 - 0s - loss: 0.4862 - accuracy: 0.7866 - val_loss: 0.5864 - val_accuracy: 0.7208\n",
      "Epoch 149/150\n",
      "62/62 - 0s - loss: 0.4822 - accuracy: 0.7785 - val_loss: 0.5735 - val_accuracy: 0.7143\n",
      "Epoch 150/150\n",
      "62/62 - 0s - loss: 0.4753 - accuracy: 0.7964 - val_loss: 0.5672 - val_accuracy: 0.7208\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(data, label, epochs=150, batch_size=10, \n",
    "                    validation_split = 0.2,  # 劃分資料集的20%作為驗證集用 \n",
    "                    verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cd71f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history: {'loss': [2.0576703548431396, 0.8917844891548157, 0.7947136163711548, 0.7492638230323792, 0.7160550951957703, 0.685688853263855, 0.6603339314460754, 0.6520437598228455, 0.6490398645401001, 0.6507017016410828, 0.6348869204521179, 0.6338370442390442, 0.6247356534004211, 0.618000328540802, 0.6256620287895203, 0.615386426448822, 0.611086905002594, 0.6023130416870117, 0.6039235591888428, 0.5996115207672119, 0.596635639667511, 0.5958499908447266, 0.593828558921814, 0.602578341960907, 0.5888012647628784, 0.578639030456543, 0.5754804015159607, 0.5784035325050354, 0.5759625434875488, 0.5676376819610596, 0.5692019462585449, 0.5815424919128418, 0.5604845285415649, 0.562128484249115, 0.5707480907440186, 0.5727440714836121, 0.5704574584960938, 0.5564420819282532, 0.5681372284889221, 0.5519600510597229, 0.5561438202857971, 0.5578890442848206, 0.5611892938613892, 0.5518606305122375, 0.5560075044631958, 0.5540204644203186, 0.5452053546905518, 0.538526713848114, 0.5502395629882812, 0.5496149659156799, 0.5481497049331665, 0.5523115992546082, 0.5491653084754944, 0.5388455986976624, 0.540246307849884, 0.5453497171401978, 0.532971203327179, 0.5502920150756836, 0.5318577289581299, 0.5353930592536926, 0.5336409211158752, 0.5356501340866089, 0.5378960967063904, 0.5287655591964722, 0.5291455984115601, 0.5279901623725891, 0.5327494144439697, 0.5392640829086304, 0.5286284685134888, 0.5385797619819641, 0.5258450508117676, 0.5367196798324585, 0.5334397554397583, 0.5319142937660217, 0.5308952927589417, 0.5213954448699951, 0.5272642970085144, 0.5245456695556641, 0.5351061224937439, 0.5262991189956665, 0.5223881006240845, 0.5307047367095947, 0.5144577026367188, 0.5117784142494202, 0.5158621072769165, 0.5293560028076172, 0.5162472724914551, 0.5168951749801636, 0.5217849612236023, 0.5173212885856628, 0.5099072456359863, 0.512586772441864, 0.504413902759552, 0.5061586499214172, 0.5162058472633362, 0.5079290866851807, 0.507735550403595, 0.5277896523475647, 0.5013139843940735, 0.5109231472015381, 0.5381823182106018, 0.5095946788787842, 0.5085952877998352, 0.501209557056427, 0.5055030584335327, 0.5089640021324158, 0.5113690495491028, 0.5012490749359131, 0.4960559904575348, 0.5015962719917297, 0.5089713335037231, 0.49581411480903625, 0.4978936016559601, 0.5076895356178284, 0.5119816064834595, 0.5004374980926514, 0.5003075003623962, 0.49973568320274353, 0.4902898967266083, 0.5030021667480469, 0.4956481158733368, 0.4956064522266388, 0.49580657482147217, 0.4901813566684723, 0.48827415704727173, 0.4956575036048889, 0.49272066354751587, 0.5046310424804688, 0.4843127131462097, 0.4929910898208618, 0.4872041344642639, 0.49234625697135925, 0.491391658782959, 0.5013333559036255, 0.5174024701118469, 0.5215287804603577, 0.4967511296272278, 0.4883502423763275, 0.4852997660636902, 0.49574369192123413, 0.48753735423088074, 0.4825069308280945, 0.4773402512073517, 0.495025634765625, 0.49066898226737976, 0.4855543375015259, 0.4884008765220642, 0.4862448275089264, 0.4822327196598053, 0.47534647583961487], 'accuracy': [0.5749185681343079, 0.6449511647224426, 0.638436496257782, 0.6482084393501282, 0.6400651335716248, 0.662866473197937, 0.6644951105117798, 0.662866473197937, 0.6775244474411011, 0.6661238074302673, 0.6758957505226135, 0.6726384162902832, 0.6710097789764404, 0.6726384162902832, 0.6644951105117798, 0.6726384162902832, 0.6856677532196045, 0.708469033241272, 0.6824104189872742, 0.7100977301597595, 0.6986970901489258, 0.6986970901489258, 0.7019544243812561, 0.7182410359382629, 0.7117263674736023, 0.7166123986244202, 0.7166123986244202, 0.7166123986244202, 0.7068403959274292, 0.723127007484436, 0.7263843417167664, 0.742671012878418, 0.7377850413322449, 0.7410423159599304, 0.723127007484436, 0.7263843417167664, 0.7394136786460876, 0.7280130386352539, 0.7198697328567505, 0.732899010181427, 0.7394136786460876, 0.7345277070999146, 0.7361563444137573, 0.7182410359382629, 0.7410423159599304, 0.732899010181427, 0.7442996501922607, 0.7361563444137573, 0.7377850413322449, 0.742671012878418, 0.723127007484436, 0.732899010181427, 0.7361563444137573, 0.7459283471107483, 0.7459283471107483, 0.7491856813430786, 0.7589576840400696, 0.7263843417167664, 0.7442996501922607, 0.7605863213539124, 0.7491856813430786, 0.7589576840400696, 0.7410423159599304, 0.7508143186569214, 0.742671012878418, 0.7410423159599304, 0.7459283471107483, 0.7442996501922607, 0.7605863213539124, 0.7296416759490967, 0.7654722929000854, 0.7475569844245911, 0.7410423159599304, 0.7491856813430786, 0.7410423159599304, 0.7638436555862427, 0.7442996501922607, 0.7491856813430786, 0.7475569844245911, 0.742671012878418, 0.757328987121582, 0.7475569844245911, 0.7491856813430786, 0.7540716528892517, 0.7605863213539124, 0.7361563444137573, 0.7687296271324158, 0.7605863213539124, 0.7508143186569214, 0.7540716528892517, 0.7508143186569214, 0.7638436555862427, 0.7654722929000854, 0.7491856813430786, 0.757328987121582, 0.7589576840400696, 0.7589576840400696, 0.7540716528892517, 0.7736156582832336, 0.7524430155754089, 0.742671012878418, 0.7785016298294067, 0.7622149586677551, 0.7752442955970764, 0.7703583240509033, 0.7410423159599304, 0.757328987121582, 0.7622149586677551, 0.7719869613647461, 0.767100989818573, 0.7589576840400696, 0.7719869613647461, 0.7622149586677551, 0.7557003498077393, 0.757328987121582, 0.757328987121582, 0.7719869613647461, 0.7638436555862427, 0.7785016298294067, 0.7638436555862427, 0.7589576840400696, 0.7605863213539124, 0.7719869613647461, 0.7703583240509033, 0.767100989818573, 0.7508143186569214, 0.767100989818573, 0.7589576840400696, 0.7654722929000854, 0.7736156582832336, 0.7654722929000854, 0.7719869613647461, 0.7605863213539124, 0.7638436555862427, 0.7589576840400696, 0.7557003498077393, 0.7557003498077393, 0.7801302671432495, 0.776872992515564, 0.7524430155754089, 0.7817589640617371, 0.7719869613647461, 0.776872992515564, 0.7638436555862427, 0.7817589640617371, 0.7736156582832336, 0.7524430155754089, 0.7866449356079102, 0.7785016298294067, 0.7964169383049011], 'val_loss': [1.0439484119415283, 0.9238125681877136, 0.8354170918464661, 0.7968044281005859, 0.7409866452217102, 0.7099510431289673, 0.7032554745674133, 0.7319667935371399, 0.6951654553413391, 0.6685201525688171, 0.6959181427955627, 0.6597435474395752, 0.6635296940803528, 0.6593634486198425, 0.6538566946983337, 0.6610419750213623, 0.6412047147750854, 0.6386898159980774, 0.6415141820907593, 0.6309658885002136, 0.6253136992454529, 0.6119937896728516, 0.6143948435783386, 0.611352801322937, 0.607642412185669, 0.599281907081604, 0.5964619517326355, 0.6351968050003052, 0.6041180491447449, 0.6108787059783936, 0.5935783982276917, 0.6120278239250183, 0.5874418616294861, 0.6150248646736145, 0.5920448899269104, 0.5986514687538147, 0.590150773525238, 0.6160501837730408, 0.5864959359169006, 0.6392110586166382, 0.5781641602516174, 0.6235435009002686, 0.5893713235855103, 0.5818691253662109, 0.5820783376693726, 0.5948843955993652, 0.6011964678764343, 0.6151134967803955, 0.6119982004165649, 0.5853716731071472, 0.6929677724838257, 0.5878839492797852, 0.5936040282249451, 0.5902441740036011, 0.5809316039085388, 0.5935282707214355, 0.6206170320510864, 0.5729518532752991, 0.5784746408462524, 0.5703538656234741, 0.5823150873184204, 0.5825294852256775, 0.5790279507637024, 0.5831065773963928, 0.5752111673355103, 0.5720446705818176, 0.5844815969467163, 0.5843614935874939, 0.592472493648529, 0.5761454105377197, 0.5826669335365295, 0.5963298678398132, 0.5968216061592102, 0.5765379071235657, 0.5759037733078003, 0.6305056214332581, 0.5950342416763306, 0.5770348310470581, 0.5791426301002502, 0.5799620747566223, 0.5737100839614868, 0.5810294151306152, 0.5911719799041748, 0.6009983420372009, 0.6478853225708008, 0.5712193846702576, 0.5859776139259338, 0.5686591267585754, 0.5968874096870422, 0.5763552188873291, 0.5901417136192322, 0.5761784911155701, 0.5735131502151489, 0.5669342875480652, 0.5696730613708496, 0.6161736845970154, 0.5850527286529541, 0.6021479368209839, 0.586592972278595, 0.5757896304130554, 0.5560358762741089, 0.5751869082450867, 0.5672972202301025, 0.5711922645568848, 0.5652951598167419, 0.6293843984603882, 0.5774012207984924, 0.6046603322029114, 0.5726147294044495, 0.5745031833648682, 0.6325322985649109, 0.5718778967857361, 0.5672259330749512, 0.564842939376831, 0.5882099866867065, 0.5690442323684692, 0.5942326188087463, 0.5807607769966125, 0.5688927173614502, 0.5911741852760315, 0.5650305151939392, 0.5645026564598083, 0.5823448300361633, 0.5638782382011414, 0.5608736276626587, 0.5597051978111267, 0.5677793622016907, 0.6136916875839233, 0.625800371170044, 0.568149745464325, 0.5685349106788635, 0.5560972690582275, 0.5640931129455566, 0.6142374277114868, 0.6425145864486694, 0.6096950173377991, 0.5735384225845337, 0.5593885183334351, 0.6011736392974854, 0.5811038613319397, 0.5786610245704651, 0.5729103684425354, 0.568661630153656, 0.6080201268196106, 0.5664897561073303, 0.5697211027145386, 0.5721702575683594, 0.5863701701164246, 0.5734806060791016, 0.5671585202217102], 'val_accuracy': [0.6103895902633667, 0.5974025726318359, 0.5649350881576538, 0.6233766078948975, 0.6558441519737244, 0.6103895902633667, 0.5649350881576538, 0.5194805264472961, 0.5454545617103577, 0.6818181872367859, 0.6558441519737244, 0.6753246784210205, 0.5974025726318359, 0.6688311696052551, 0.6558441519737244, 0.6103895902633667, 0.6818181872367859, 0.6753246784210205, 0.6558441519737244, 0.6558441519737244, 0.6558441519737244, 0.6948052048683167, 0.6818181872367859, 0.6818181872367859, 0.7077922224998474, 0.6883116960525513, 0.6883116960525513, 0.6753246784210205, 0.6753246784210205, 0.6883116960525513, 0.6948052048683167, 0.6753246784210205, 0.6623376607894897, 0.6818181872367859, 0.6623376607894897, 0.6948052048683167, 0.6753246784210205, 0.6623376607894897, 0.6623376607894897, 0.6753246784210205, 0.6688311696052551, 0.6753246784210205, 0.6818181872367859, 0.6688311696052551, 0.6558441519737244, 0.6623376607894897, 0.6688311696052551, 0.6948052048683167, 0.6883116960525513, 0.6818181872367859, 0.6623376607894897, 0.6558441519737244, 0.6948052048683167, 0.6623376607894897, 0.6688311696052551, 0.6753246784210205, 0.6883116960525513, 0.701298713684082, 0.6883116960525513, 0.7077922224998474, 0.6818181872367859, 0.6948052048683167, 0.6883116960525513, 0.6948052048683167, 0.7077922224998474, 0.701298713684082, 0.6948052048683167, 0.6883116960525513, 0.701298713684082, 0.701298713684082, 0.6818181872367859, 0.6753246784210205, 0.7142857313156128, 0.7077922224998474, 0.7142857313156128, 0.6948052048683167, 0.6688311696052551, 0.7077922224998474, 0.7402597665786743, 0.6883116960525513, 0.7077922224998474, 0.7077922224998474, 0.701298713684082, 0.6948052048683167, 0.6753246784210205, 0.7207792401313782, 0.701298713684082, 0.7077922224998474, 0.6948052048683167, 0.7077922224998474, 0.6948052048683167, 0.7402597665786743, 0.7077922224998474, 0.7337662577629089, 0.7077922224998474, 0.6948052048683167, 0.7142857313156128, 0.6753246784210205, 0.701298713684082, 0.7142857313156128, 0.7467532753944397, 0.7207792401313782, 0.7207792401313782, 0.7272727489471436, 0.7272727489471436, 0.6753246784210205, 0.7077922224998474, 0.6818181872367859, 0.7337662577629089, 0.7142857313156128, 0.6623376607894897, 0.7207792401313782, 0.7207792401313782, 0.7207792401313782, 0.6883116960525513, 0.7337662577629089, 0.7142857313156128, 0.701298713684082, 0.7207792401313782, 0.7142857313156128, 0.7142857313156128, 0.7207792401313782, 0.701298713684082, 0.7077922224998474, 0.7077922224998474, 0.7337662577629089, 0.7272727489471436, 0.6623376607894897, 0.6883116960525513, 0.7337662577629089, 0.7207792401313782, 0.7337662577629089, 0.7272727489471436, 0.6883116960525513, 0.6883116960525513, 0.6818181872367859, 0.701298713684082, 0.7272727489471436, 0.7077922224998474, 0.7207792401313782, 0.7207792401313782, 0.7207792401313782, 0.7207792401313782, 0.7142857313156128, 0.7467532753944397, 0.7337662577629089, 0.7207792401313782, 0.7207792401313782, 0.7142857313156128, 0.7207792401313782]}\n"
     ]
    }
   ],
   "source": [
    "print(\"history:\", history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84b25b",
   "metadata": {},
   "source": [
    "## 4. 評估與預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2aff8760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 3ms/step - loss: 0.4892 - accuracy: 0.7643\n",
      "\n",
      "Loss: 0.49, Accuracy: 76.43%\n",
      "Prediction Accuracy: 76.43%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(data, label)\n",
    "print(\"\\nLoss: {:.2}, Accuracy: {:.2%}\".format(loss, accuracy))\n",
    "\n",
    "probabilities = model.predict(data)\n",
    "\n",
    "# 將 probabilities 的輸出值透過np.round()做四捨五入, 會得到 0 或 1\n",
    "predictions = [float(np.round(x)) for x in probabilities]\n",
    "\n",
    "# 計算預測結果跟真實結果的平均差距\n",
    "accuracy = np.mean(predictions == label)\n",
    "print(\"Prediction Accuracy: {:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814fb67d",
   "metadata": {},
   "source": [
    "### note: \n\n",
    "將網路隱藏層加深一層model.add(Dense(4, activation='relu'))\n\n",
    "評估損失值及準確度由\n\n",
    "Loss: 0.54, Accuracy: 74.22%\n\n",
    "優化至\n\n",
    "Loss: 0.49, Accuracy: 76.43%\n\n",
    "將epochs從150提升為200, 評估損失值及準確度優化至\n\n",
    "Loss: 0.44, Accuracy: 78.78%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
